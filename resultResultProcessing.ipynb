{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from modules import helper\n",
    "from scipy import stats\n",
    "from scipy.stats.stats import pearsonr\n",
    "from modules import saliencyHelper as sh\n",
    "\n",
    "#from sacred import Experiment\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modules import dataset_selecter as ds\n",
    "from modules import pytorchTrain as pt\n",
    "from modules import saliencyHelper as sh\n",
    "from modules import helper\n",
    "#import seml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    def __init__(self, configName, hyperss, hypNames, test_sizes, topLevelss, toplevels, dataset, symbols, nrEmpty,andStack,orStack,xorStack,nrAnds,nrOrs,nrxor,trueIndexes,orOffSet,xorOffSet,redaundantIndexes,batch_size):\n",
    "        self.configName=configName\n",
    "        self.hyperss=hyperss\n",
    "        self.hypNames=hypNames\n",
    "        self.test_sizes=test_sizes\n",
    "        self.topLevelss=topLevelss\n",
    "        self.toplevels=toplevels\n",
    "        self.dataset=dataset\n",
    "        self.symbols=symbols\n",
    "        self.nrEmpty=nrEmpty\n",
    "        self.andStack=andStack\n",
    "        self.orStack=orStack\n",
    "        self.xorStack=xorStack\n",
    "        self.nrAnds=nrAnds\n",
    "        self.nrOrs=nrOrs\n",
    "        self.nrxor=nrxor\n",
    "        self.trueIndexes=trueIndexes\n",
    "        self.orOffSet=orOffSet\n",
    "        self.xorOffSet=xorOffSet\n",
    "        self.redaundantIndexes=redaundantIndexes\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "allConfigs = []\n",
    "\n",
    "hyperss = [['Transformer', 500, 64, 0.5, True, True, 4 ,  2, 0 , 0, True]\n",
    "    , ['Transformer', 500, 64, 0.5, True, True, 4 ,  2, 0 , 0.3, True]\n",
    "    , ['Transformer', 500, 64, 0.5, True, True, 4 ,  2, 0 , 0, False]\n",
    "    , ['Transformer', 500, 64, 0.5, True, True, 4 ,  2, 0 , 0.3, False]\n",
    "    , ['CNN', 500, 8, 1, True , True , 0 ,  2, 0 , 0, False]\n",
    "    , ['CNN', 500, 8, 1, False , True , 0 ,  2, 0 , 0, False]\n",
    "    , ['CNN', 500, 8, 1, True , False , 0 ,  2, 0 , 0, False]\n",
    "    , ['CNN', 500, 8, 1, False , False , 0 ,  2, 0 , 0, False]\n",
    "    ]\n",
    "\n",
    "hypNames= 'TC'\n",
    "\n",
    "#NOTE: Change test_sizes for all configs to remove some of them from the results (not applicable for all print)\n",
    "test_sizes = [0,0.1]\n",
    "topLevelss = [['or', 'and', 'xor'],['and'], ['or'],['xor']]\n",
    "toplevels = ['or', 'and', 'xor']\n",
    "\n",
    "dataset = 'Andor'\n",
    "symbols = 2#4\n",
    "nrEmpty = 2#2\n",
    "andStack = 1\n",
    "orStack = 1\n",
    "xorStack = 1\n",
    "nrAnds = 2#2\n",
    "nrOrs = 2#2\n",
    "nrxor = 2#2\n",
    "trueIndexes = [1]#[1]#[1,3]\n",
    "orOffSet = 0\n",
    "xorOffSet = 0\n",
    "redaundantIndexes = []\n",
    "batch_size = 10 #100#500#10\n",
    "configName = '2inBinary'\n",
    "\n",
    "in2Config = config(configName, hyperss, hypNames, test_sizes, topLevelss, toplevels, dataset, symbols, nrEmpty,andStack,orStack,xorStack,nrAnds,nrOrs,nrxor,trueIndexes,orOffSet,xorOffSet,redaundantIndexes,batch_size)\n",
    "allConfigs.append(in2Config)\n",
    "\n",
    "hyperss = [['Transformer', 150, 32, 0.5, True, True, 4 ,  2, 0 , 0, True]\n",
    "    , ['Transformer', 150, 32, 0.5, True, True, 4 ,  2, 0 , 0.3, True]\n",
    "    , ['Transformer', 150, 32, 0.5, True, True, 4 ,  2, 0 , 0, False]\n",
    "    , ['Transformer', 150, 32, 0.5, True, True, 4 ,  2, 0 , 0.3, False]\n",
    "    , ['CNN', 100, 8, 1, True , True , 0 ,  2, 0 , 0, False]\n",
    "    , ['CNN', 100, 8, 1, False , True , 0 ,  2, 0 , 0, False]\n",
    "    , ['CNN', 100, 8, 1, True , False , 0 ,  2, 0 , 0, False]\n",
    "    , ['CNN', 100, 8, 1, False , False , 0 ,  2, 0 , 0, False]\n",
    "    ]\n",
    "\n",
    "hypNames= 'TC'\n",
    "\n",
    "test_sizes = [0,0.2]\n",
    "topLevelss = [['or', 'and', 'xor'],['and'], ['or'],['xor']]\n",
    "toplevels = ['or', 'and', 'xor']\n",
    "\n",
    "dataset = 'Andor'\n",
    "symbols = 4#4\n",
    "nrEmpty = 2#2\n",
    "andStack = 1\n",
    "orStack = 1\n",
    "xorStack = 1\n",
    "nrAnds = 2#2\n",
    "nrOrs = 2#2\n",
    "nrxor = 2#2\n",
    "trueIndexes = [1,3]#[1]#[1,3]\n",
    "orOffSet = 0\n",
    "xorOffSet = 0\n",
    "redaundantIndexes = []\n",
    "batch_size = 500 #100#500#10\n",
    "configName = '2inQuaternary'\n",
    "\n",
    "in2s4Config = config(configName, hyperss, hypNames, test_sizes, topLevelss, toplevels, dataset, symbols, nrEmpty,andStack,orStack,xorStack,nrAnds,nrOrs,nrxor,trueIndexes,orOffSet,xorOffSet,redaundantIndexes,batch_size)\n",
    "allConfigs.append(in2s4Config)\n",
    "\n",
    "hyperss = [['Transformer', 150, 32, 0.5, True, True, 4 ,  2, 0 , 0, True]\n",
    "    , ['Transformer', 150, 32, 0.5, True, True, 4 ,  2, 0 , 0.3, True]\n",
    "    , ['Transformer', 150, 32, 0.5, True, True, 4 ,  2, 0 , 0, False]\n",
    "    , ['Transformer', 150, 32, 0.5, True, True, 4 ,  2, 0 , 0.3, False]\n",
    "    , ['CNN', 100, 8, 1, True , True , 0 ,  2, 0 , 0, False]\n",
    "    , ['CNN', 100, 8, 1, False , True , 0 ,  2, 0 , 0, False]\n",
    "    , ['CNN', 100, 8, 1, True , False , 0 ,  2, 0 , 0, False]\n",
    "    , ['CNN', 100, 8, 1, False , False , 0 ,  2, 0 , 0, False]\n",
    "    ]\n",
    "\n",
    "hypNames= 'TC'\n",
    "configName = '3inBinary'\n",
    "\n",
    "test_sizes = [0,0.2]\n",
    "topLevelss = [['or', 'and', 'xor'],['and'], ['or'],['xor']]\n",
    "toplevels = ['or', 'and', 'xor']\n",
    "\n",
    "dataset = 'Andor'\n",
    "symbols = 2#4\n",
    "nrEmpty = 3#2\n",
    "andStack = 1\n",
    "orStack = 1\n",
    "xorStack = 1\n",
    "nrAnds = 3#2\n",
    "nrOrs = 3#2\n",
    "nrxor = 3#2\n",
    "trueIndexes = [1]#[1]#[1,3]\n",
    "orOffSet = 0\n",
    "xorOffSet = 0\n",
    "redaundantIndexes = []\n",
    "batch_size = 100 #100#500#10\n",
    "\n",
    "in3Config = config(configName, hyperss, hypNames, test_sizes, topLevelss, toplevels, dataset, symbols, nrEmpty,andStack,orStack,xorStack,nrAnds,nrOrs,nrxor,trueIndexes,orOffSet,xorOffSet,redaundantIndexes,batch_size)\n",
    "allConfigs.append(in3Config)\n",
    "\n",
    "\n",
    "filteredResultsDir = 'filteredResults'\n",
    "\n",
    "folder = './Bilder/' + dataset+'-m'+hypNames+'-s'+str(symbols)+'-stacks'+str(andStack)+str(orStack)+str(xorStack)+'-sizes'+str(nrAnds)+str(nrOrs)+str(nrxor)+str(nrEmpty)+'-mods'+str(trueIndexes)+str(orOffSet)+str(xorOffSet)+str(redaundantIndexes)+'-model'+str(batch_size)+str(test_sizes)+'/'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "folderGeneral = './Bilder/general/'\n",
    "if not os.path.exists(folderGeneral):\n",
    "    os.makedirs(folderGeneral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accTreshold = 0 # or 1 if only 100\\% data should be considert!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrayToString(indexes):\n",
    "    out = \"\"\n",
    "    for i in indexes:\n",
    "        out = out + ',' + str(i)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topLevelNames = ['all','and','or', 'xor']\n",
    "fig, axs1 = plt.subplots(nrows=1, ncols=3, sharex=True, sharey=True,\n",
    "                                    figsize=(12, 1))\n",
    "\n",
    "fig2, axs2 = plt.subplots(nrows=1, ncols=3, sharex=True, sharey=True,\n",
    "                                    figsize=(12, 1))\n",
    "\n",
    "allRes = dict()\n",
    "\n",
    "accuracyThreshold = 1 # set to 0 if all data should be used\n",
    "\n",
    "for j, toplevels in enumerate(topLevelss[1:]):\n",
    "    ax1 = axs1[j]\n",
    "    ax2 = axs2[j]\n",
    "    resultV1s = []\n",
    "    resultV2s = []\n",
    "    resultV3s = [[],[],[],[]]\n",
    "    resultV4s = []\n",
    "\n",
    "\n",
    "    for config in allConfigs:\n",
    "        hyperss = config.hyperss\n",
    "        hypNames= config.hypNames\n",
    "        test_sizes = config.test_sizes\n",
    "        #topLevelss = config.topLevelss\n",
    "        #toplevels = config.toplevels\n",
    "        dataset = config.dataset\n",
    "        symbols = config.symbols\n",
    "        nrEmpty = config.nrEmpty\n",
    "        andStack = config.andStack\n",
    "        orStack = config.orStack\n",
    "        xorStack = config.xorStack\n",
    "        nrAnds = config.nrAnds\n",
    "        nrOrs = config.nrOrs\n",
    "        nrxor = config.nrxor\n",
    "        trueIndexes = config.trueIndexes\n",
    "        orOffSet = config.orOffSet\n",
    "        xorOffSet = config.xorOffSet\n",
    "        redaundantIndexes = config.redaundantIndexes\n",
    "        batch_size = config.batch_size\n",
    "\n",
    "        for hypers in hyperss:\n",
    "            for toplevel in toplevels:\n",
    "                for test_size in test_sizes:\n",
    "                    modelType = hypers[0]\n",
    "                    epochs = hypers[1]\n",
    "                    dmodel = hypers[2]\n",
    "                    dfff = hypers[3]\n",
    "                    doSkip = hypers[4]\n",
    "                    doBn = hypers[5]\n",
    "                    header = hypers[6]\n",
    "                    numOfLayers = hypers[7]\n",
    "                    dropout = hypers[8]\n",
    "                    att_dropout = hypers[9]\n",
    "                    if modelType == 'Transformer':\n",
    "                        doClsTocken = hypers[10]\n",
    "                    else:\n",
    "                        doClsTocken = False\n",
    "\n",
    "                    dsName = str(dataset) +  ',l' + str(toplevel) +',s' +  str(symbols)+',e' +  str(nrEmpty)+',a' +  str(andStack)+',o' +  str(orStack)+',x' +  str(xorStack)+',na' +  str(nrAnds)+',no' +  str(nrOrs)+',nx' +  str(nrxor)+',i' +  arrayToString(trueIndexes)+',t' +  str(test_size)+',oo' +  str(orOffSet)+',xo' +  str(xorOffSet) +',r' + arrayToString(redaundantIndexes)\n",
    "\n",
    "                    saveName = pt.getWeightName(dsName, dataset, batch_size, epochs, numOfLayers, header, dmodel, dfff, dropout, att_dropout, doSkip, doBn, doClsTocken,learning = False, results = True, resultsPath=filteredResultsDir)\n",
    "\n",
    "                    if os.path.isfile(saveName + '.pkl'):\n",
    "\n",
    "                        results = helper.load_obj(saveName)\n",
    "\n",
    "                        res = dict()\n",
    "                        for index, v in np.ndenumerate(results):\n",
    "                            res = v\n",
    "                        \n",
    "                        name = 'performance'\n",
    "                        v  = res[name]\n",
    "                        allRes[saveName] = res\n",
    "                        \n",
    "                        for i, val in enumerate(v):\n",
    "                            if val >= accTreshold:\n",
    "                                resultV1s.append(val)\n",
    "                                resultV2s.append(res['tree performance'][i])\n",
    "                                for r in res['treeImportances'][i]:\n",
    "                                    for k in range(nrAnds):\n",
    "                                        resultV3s[0].append(res['treeImportances'][i][k])\n",
    "                                    for k in range(nrAnds, nrAnds+nrOrs):\n",
    "                                        resultV3s[1].append(res['treeImportances'][i][k])\n",
    "                                    for k in range(nrAnds+nrOrs, nrAnds+nrOrs+nrxor):\n",
    "                                        resultV3s[2].append(res['treeImportances'][i][k])\n",
    "                                    for k in range(nrAnds+nrOrs+nrxor, nrAnds+nrOrs+nrxor+nrEmpty):\n",
    "                                        resultV3s[3].append(res['treeImportances'][i][k])\n",
    "\n",
    "                            \n",
    "                                if res['baseline'][0] < 0.5:\n",
    "                                    baseline = 1-res['baseline'][0]\n",
    "                                else:\n",
    "                                    baseline = res['baseline'][0]\n",
    "                                resultV4s.append(baseline)\n",
    "                    else:\n",
    "                        print('cant find: ' + saveName)\n",
    "\n",
    "    print(str(np.round(np.mean(resultV1s, axis=0),4)) + ' +- ' + str(np.round(np.std(resultV1s, axis=0),4)))\n",
    "    print(str(np.round(np.mean(resultV2s, axis=0),4)) + ' +- ' + str(np.round(np.std(resultV2s, axis=0),4)))\n",
    "    print(str(np.round(np.mean(resultV3s, axis=1),4)) + ' +- ' + str(np.round(np.std(resultV3s, axis=1),4)))\n",
    "    print(str(np.round(np.mean(resultV4s, axis=0),4)) + ' +- ' + str(np.round(np.std(resultV4s, axis=0),4)))\n",
    "    print('------')\n",
    "\n",
    "\n",
    "    lables = ['Avg. Model Acc.', 'Avg Tree Acc.', 'Avg. Baseline Acc.']\n",
    "    counts = [np.round(np.mean(resultV1s, axis=0),4),np.round(np.mean(resultV2s, axis=0),4),np.round(np.mean(resultV4s, axis=0),4)]\n",
    "    e =  [np.round(np.std(resultV1s, axis=0),4),np.round(np.std(resultV2s, axis=0),4),np.round(np.std(resultV4s, axis=0),4)]\n",
    "\n",
    "    ax1.bar(lables, counts, yerr=e, linestyle='None', capsize=4)\n",
    "    ax1.tick_params(labelrotation=90)\n",
    "\n",
    "    ax1.set_ylabel('Acc.')\n",
    "    ax1.set_title('Model Performance ' +topLevelNames[j].upper())\n",
    "    \n",
    "\n",
    "\n",
    "    lables= []\n",
    "    for i in range(andStack):\n",
    "        lables.append('AND')\n",
    "    for i in range(orStack):\n",
    "        lables.append('OR'+str(i))\n",
    "    for i in range(xorStack):\n",
    "        lables.append('XOR')\n",
    "    for i in range(1):\n",
    "        lables.append('Baseline')\n",
    "        \n",
    "    counts = np.round(np.mean(resultV3s, axis=1),4)\n",
    "    e =  np.round(np.std(resultV3s, axis=1),4)\n",
    "\n",
    "    ax2.bar(lables, counts, yerr=e, linestyle='None', capsize=4)\n",
    "    ax2.tick_params(labelrotation=90)\n",
    "    ax2.set_ylabel('Importance')\n",
    "    ax2.set_title('Tree Model Importance ' + topLevelNames[j+1].upper())\n",
    "\n",
    "fig.savefig(folderGeneral + 'generalPerformancesGates.png', dpi = 300, bbox_inches = 'tight')\n",
    "fig2.savefig(folderGeneral + 'generalTreesImportancesGates.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topLevelNamesSmall = [['and'],['or'],['xor']]\n",
    "kNames = ['LRP-Full', 'LRP-Rollout', 'LRP-Transformer without CLS', 'LRP-Transformer with CLS', 'IntegratedGradients', 'DeepLift', 'KernelSHAP', 'GuidedGradCam', 'FeaturePermutation', 'Deconvolution','GradCAM++', 'GradCAM']\n",
    "kIter = ['LRP-full', 'LRP-rollout', 'LRP-transformer_attribution', 'LRP-transformer_attribution cls', 'captum-IntegratedGradients', 'captum-DeepLift', 'captum-KernelShap', 'captum-GuidedGradCam', 'captum-FeaturePermutation', 'captum-Deconvolution','PytGradCam-GradCAMPlusPlus', 'PytGradCam-GradCAM']\n",
    "for kNr, k in enumerate(kIter):\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=3, sharex=True,# sharey=True,\n",
    "                                figsize=(12, 6))\n",
    "\n",
    "    if k == 'LRP-transformer_attribution cls':\n",
    "        k = 'LRP-transformer_attribution'\n",
    "        takeCls = True\n",
    "    else:\n",
    "        takeCls = False\n",
    "\n",
    "    for j, toplevels in enumerate(topLevelNamesSmall):\n",
    "        for ci, c in enumerate([0,1,2]):\n",
    "\n",
    "            ax = axs[ci, j]\n",
    "            ax.ticklabel_format(style='sci', axis='y', scilimits=(-3,3), useMathText=False)\n",
    "\n",
    "            resultVs = [[],[],[],[]]\n",
    "            for cNr, config in enumerate(allConfigs):\n",
    "                configName = config.configName\n",
    "                hyperss = config.hyperss\n",
    "                hypNames= config.hypNames\n",
    "                test_sizes = config.test_sizes\n",
    "                topLevelss = config.topLevelss\n",
    "                dataset = config.dataset\n",
    "                symbols = config.symbols\n",
    "                nrEmpty = config.nrEmpty\n",
    "                andStack = config.andStack\n",
    "                orStack = config.orStack\n",
    "                xorStack = config.xorStack\n",
    "                nrAnds = config.nrAnds\n",
    "                nrOrs = config.nrOrs\n",
    "                nrxor = config.nrxor\n",
    "                trueIndexes = config.trueIndexes\n",
    "                orOffSet = config.orOffSet\n",
    "                xorOffSet = config.xorOffSet\n",
    "                redaundantIndexes = config.redaundantIndexes\n",
    "                batch_size = config.batch_size\n",
    "\n",
    " \n",
    "\n",
    "            \n",
    "\n",
    "                if k in ['performance', 'tree performance', 'treeImportances', 'baseline']:\n",
    "                    continue\n",
    "                \n",
    "                for hypers in hyperss:\n",
    "                    if takeCls and not hypers[-1]:\n",
    "                        continue\n",
    "                    elif not takeCls and hypers[-1]:\n",
    "                        continue\n",
    "                    for toplevel in toplevels:\n",
    "                        for test_size in test_sizes:\n",
    "                            modelType = hypers[0]\n",
    "                            epochs = hypers[1]\n",
    "                            dmodel = hypers[2]\n",
    "                            dfff = hypers[3]\n",
    "                            doSkip = hypers[4]\n",
    "                            doBn = hypers[5]\n",
    "                            header = hypers[6]\n",
    "                            numOfLayers = hypers[7]\n",
    "                            dropout = hypers[8]\n",
    "                            att_dropout = hypers[9]\n",
    "                            if modelType == 'Transformer':\n",
    "                                doClsTocken = hypers[10]\n",
    "                            else:\n",
    "                                doClsTocken = False\n",
    "\n",
    "                            dsName = str(dataset) +  ',l' + str(toplevel) +',s' +  str(symbols)+',e' +  str(nrEmpty)+',a' +  str(andStack)+',o' +  str(orStack)+',x' +  str(xorStack)+',na' +  str(nrAnds)+',no' +  str(nrOrs)+',nx' +  str(nrxor)+',i' +  arrayToString(trueIndexes)+',t' +  str(test_size)+',oo' +  str(orOffSet)+',xo' +  str(xorOffSet) +',r' + arrayToString(redaundantIndexes)\n",
    "\n",
    "                            saveName = pt.getWeightName(dsName, dataset, batch_size, epochs, numOfLayers, header, dmodel, dfff, dropout, att_dropout, doSkip, doBn, doClsTocken,learning = False, results = True, resultsPath=filteredResultsDir)\n",
    "\n",
    "                            if os.path.isfile(saveName + '.pkl'):\n",
    "\n",
    "                                res = allRes[saveName]\n",
    "                                if k not in res.keys():\n",
    "                                    continue\n",
    "                                    \n",
    "                                name = 'saliency'\n",
    "                                if c == 2:\n",
    "                                    v  = res[k][name]\n",
    "                                else:\n",
    "                                    v  = res[k][c][name]\n",
    "                                sd = np.array(v)\n",
    "                                do2DData = False\n",
    "                                if len(sd.shape) > 2:\n",
    "                                    do2DData = True\n",
    "                                if do2DData:\n",
    "                                    v = np.sum(sd, axis=1)-1\n",
    "                                for i, val in enumerate(v):\n",
    "                                    if res['performance'][i] < accTreshold:\n",
    "                                        continue\n",
    "                                    for l in range(nrAnds):\n",
    "                                        resultVs[0].append(val[l])\n",
    "                                    for l in range(nrAnds, nrAnds+nrOrs):\n",
    "                                        resultVs[1].append(val[l])\n",
    "                                    for l in range(nrAnds+nrOrs, nrAnds+nrOrs+nrxor):\n",
    "                                        resultVs[2].append(val[l])\n",
    "                                    for l in range(nrAnds+nrOrs+nrxor, nrAnds+nrOrs+nrxor+nrEmpty):\n",
    "                                        resultVs[3].append(val[l])\n",
    "                            else:\n",
    "                                print('cant find: ' + saveName)\n",
    "            print(k)\n",
    "            print(str(np.round(np.nanmean(resultVs, axis=1),4)) + ' +- ' + str(np.round(np.nanstd(resultVs, axis=1),4)))\n",
    "            lables= []\n",
    "            for i in range(andStack):\n",
    "                lables.append('AND')\n",
    "            for i in range(orStack):\n",
    "                lables.append('OR')\n",
    "            for i in range(xorStack):\n",
    "                lables.append('XOR')\n",
    "            for i in range(1):\n",
    "                lables.append('Baseline')\n",
    "                \n",
    "            counts = np.nanmean(resultVs, axis=1)\n",
    "            e =  np.nanstd(resultVs, axis=1)\n",
    "\n",
    "            ax.bar(lables, counts, yerr=e, linestyle='None', capsize=4)\n",
    "\n",
    "            ax.set_title(topLevelNamesSmall[j][0].upper())\n",
    "            ax.xaxis_date()\n",
    "            #ax.tick_params(labelrotation=90)\n",
    "    fig.text(0.06, 0.5, 'Saliency Scores', va='center', rotation='vertical')\n",
    "    plt.suptitle(kNames[kNr])\n",
    "    \n",
    "    #fig.autofmt_xdate()\n",
    "    #fig.subplots_adjust(top=0.75)\n",
    "    fig.savefig(folderGeneral + kIter[kNr] +' GeneralAvgImportanceClasses.png', dpi = 300, bbox_inches = 'tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "topLevelNamesSmall = [['and'],['or'],['xor']]\n",
    "kNames = ['LRP-Full', 'LRP-Rollout', 'LRP-Transformer without CLS', 'LRP-Transformer with CLS', 'IntegratedGradients', 'DeepLift', 'KernelSHAP', 'GuidedGradCam', 'FeaturePermutation', 'Deconvolution','GradCAM++', 'GradCAM']\n",
    "kIter = ['LRP-full', 'LRP-rollout', 'LRP-transformer_attribution', 'LRP-transformer_attribution cls', 'captum-IntegratedGradients', 'captum-DeepLift', 'captum-KernelShap', 'captum-GuidedGradCam', 'captum-FeaturePermutation', 'captum-Deconvolution','PytGradCam-GradCAMPlusPlus', 'PytGradCam-GradCAM']\n",
    "for kNr, k in enumerate(kIter):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3, sharex=True,# sharey=True,\n",
    "                                figsize=(6, 2))\n",
    "\n",
    "\n",
    "    if k == 'LRP-transformer_attribution cls':\n",
    "        k = 'LRP-transformer_attribution'\n",
    "        takeCls = True\n",
    "    else:\n",
    "        takeCls = False\n",
    "\n",
    "    for j, toplevels in enumerate(topLevelNamesSmall):\n",
    "\n",
    "        ax = axs[j]\n",
    "        resultVs = [[],[],[],[]]\n",
    "        for cNr, config in enumerate(allConfigs):\n",
    "            configName = config.configName\n",
    "            hyperss = config.hyperss\n",
    "            hypNames= config.hypNames\n",
    "            test_sizes = config.test_sizes\n",
    "            topLevelss = config.topLevelss\n",
    "            dataset = config.dataset\n",
    "            symbols = config.symbols\n",
    "            nrEmpty = config.nrEmpty\n",
    "            andStack = config.andStack\n",
    "            orStack = config.orStack\n",
    "            xorStack = config.xorStack\n",
    "            nrAnds = config.nrAnds\n",
    "            nrOrs = config.nrOrs\n",
    "            nrxor = config.nrxor\n",
    "            trueIndexes = config.trueIndexes\n",
    "            orOffSet = config.orOffSet\n",
    "            xorOffSet = config.xorOffSet\n",
    "            redaundantIndexes = config.redaundantIndexes\n",
    "            batch_size = config.batch_size\n",
    "\n",
    " \n",
    "\n",
    "            for c in [2]:\n",
    "\n",
    "                if k in ['performance', 'tree performance', 'treeImportances', 'baseline']:\n",
    "                    continue\n",
    "                \n",
    "                for hypers in hyperss:\n",
    "                    if takeCls and not hypers[-1]:\n",
    "                        continue\n",
    "                    elif not takeCls and hypers[-1]:\n",
    "                        continue\n",
    "                    for toplevel in toplevels:\n",
    "                        for test_size in test_sizes:\n",
    "                            modelType = hypers[0]\n",
    "                            epochs = hypers[1]\n",
    "                            dmodel = hypers[2]\n",
    "                            dfff = hypers[3]\n",
    "                            doSkip = hypers[4]\n",
    "                            doBn = hypers[5]\n",
    "                            header = hypers[6]\n",
    "                            numOfLayers = hypers[7]\n",
    "                            dropout = hypers[8]\n",
    "                            att_dropout = hypers[9]\n",
    "                            if modelType == 'Transformer':\n",
    "                                doClsTocken = hypers[10]\n",
    "                            else:\n",
    "                                doClsTocken = False\n",
    "\n",
    "                            dsName = str(dataset) +  ',l' + str(toplevel) +',s' +  str(symbols)+',e' +  str(nrEmpty)+',a' +  str(andStack)+',o' +  str(orStack)+',x' +  str(xorStack)+',na' +  str(nrAnds)+',no' +  str(nrOrs)+',nx' +  str(nrxor)+',i' +  arrayToString(trueIndexes)+',t' +  str(test_size)+',oo' +  str(orOffSet)+',xo' +  str(xorOffSet) +',r' + arrayToString(redaundantIndexes)\n",
    "\n",
    "                            saveName = pt.getWeightName(dsName, dataset, batch_size, epochs, numOfLayers, header, dmodel, dfff, dropout, att_dropout, doSkip, doBn, doClsTocken,learning = False, results = True, resultsPath=filteredResultsDir)\n",
    "\n",
    "                            if os.path.isfile(saveName + '.pkl'):\n",
    "\n",
    "                                res = allRes[saveName]\n",
    "                                if k not in res.keys():\n",
    "                                    continue\n",
    "\n",
    "                                name = 'saliency'\n",
    "                                if c == 2:\n",
    "                                    v  = res[k][name]\n",
    "                                else:\n",
    "                                    v  = res[k][c][name]\n",
    "                                sd = np.array(v)\n",
    "                                do2DData = False\n",
    "                                if len(sd.shape) > 2:\n",
    "                                    do2DData = True\n",
    "                                if do2DData:\n",
    "                                    v = np.sum(sd, axis=1)-1\n",
    "                                for i, val in enumerate(v):\n",
    "                                    if res['performance'][i] < accTreshold:\n",
    "                                        continue\n",
    "                                    for l in range(nrAnds):\n",
    "                                        resultVs[0].append(val[l])\n",
    "                                    for l in range(nrAnds, nrAnds+nrOrs):\n",
    "                                        resultVs[1].append(val[l])\n",
    "                                    for l in range(nrAnds+nrOrs, nrAnds+nrOrs+nrxor):\n",
    "                                        resultVs[2].append(val[l])\n",
    "                                    for l in range(nrAnds+nrOrs+nrxor, nrAnds+nrOrs+nrxor+nrEmpty):\n",
    "                                        resultVs[3].append(val[l])\n",
    "                            else:\n",
    "                                print('cant find: ' + saveName)\n",
    "        print(k)\n",
    "        print(str(np.round(np.nanmean(resultVs, axis=1),4)) + ' +- ' + str(np.round(np.nanstd(resultVs, axis=1),4)))\n",
    "        lables= []\n",
    "        for i in range(andStack):\n",
    "            lables.append('AND')\n",
    "        for i in range(orStack):\n",
    "            lables.append('OR')\n",
    "        for i in range(xorStack):\n",
    "            lables.append('XOR')\n",
    "        for i in range(1):\n",
    "            lables.append('Baseline')\n",
    "            \n",
    "        counts = np.nanmean(resultVs, axis=1)\n",
    "        e =  np.nanstd(resultVs, axis=1)\n",
    "\n",
    "        ax.bar(lables, counts, yerr=e, linestyle='None', capsize=4)\n",
    "\n",
    "        ax.set_title(topLevelNamesSmall[j][0].upper())\n",
    "        ax.tick_params(labelrotation=90)\n",
    "    fig.text(0.04, 0.5, 'Saliency Scores', va='center', rotation='vertical')\n",
    "    plt.suptitle(kNames[kNr])\n",
    "    fig.subplots_adjust(top=0.75)\n",
    "    fig.savefig(folderGeneral +' GeneralAvgImportance' + kIter[kNr] +'.png', dpi = 300, bbox_inches = 'tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = ['LRP-full', 'LRP-rollout', 'LRP-transformer_attribution', 'captum-IntegratedGradients', 'captum-DeepLift', 'captum-KernelShap', 'captum-GuidedGradCam', 'captum-FeaturePermutation', 'captum-Deconvolution','PytGradCam-GradCAMPlusPlus', 'PytGradCam-GradCAM']\n",
    "for k in ks:\n",
    "    fig, axs1 = plt.subplots(nrows=3, ncols=4, sharex=True, sharey=True,\n",
    "                                        figsize=(12, 6))\n",
    "\n",
    "    fig2, axs2 = plt.subplots(nrows=3, ncols=4, sharex=True, sharey=True,\n",
    "                                        figsize=(12, 6))\n",
    "\n",
    "    for c in [0,1,2]:\n",
    "        for j, toplevels in enumerate(topLevelss):\n",
    "\n",
    "            ax1 = axs1[c,j]\n",
    "            ax2 = axs2[c,j]\n",
    "            resultVs = []\n",
    "            resultVMeans = []\n",
    "\n",
    "            if k in ['performance', 'tree performance', 'treeImportances', 'baseline']:\n",
    "                continue\n",
    "\n",
    "\n",
    "            for hypers in hyperss:\n",
    "                for toplevel in toplevels:\n",
    "                    for test_size in test_sizes:\n",
    "                        modelType = hypers[0]\n",
    "                        epochs = hypers[1]\n",
    "                        dmodel = hypers[2]\n",
    "                        dfff = hypers[3]\n",
    "                        doSkip = hypers[4]\n",
    "                        doBn = hypers[5]\n",
    "                        header = hypers[6]\n",
    "                        numOfLayers = hypers[7]\n",
    "                        dropout = hypers[8]\n",
    "                        att_dropout = hypers[9]\n",
    "                        if modelType == 'Transformer':\n",
    "                            doClsTocken = hypers[10]\n",
    "                        else:\n",
    "                            doClsTocken = False\n",
    "\n",
    "                        dsName = str(dataset) +  ',l' + str(toplevel) +',s' +  str(symbols)+',e' +  str(nrEmpty)+',a' +  str(andStack)+',o' +  str(orStack)+',x' +  str(xorStack)+',na' +  str(nrAnds)+',no' +  str(nrOrs)+',nx' +  str(nrxor)+',i' +  arrayToString(trueIndexes)+',t' +  str(test_size)+',oo' +  str(orOffSet)+',xo' +  str(xorOffSet) +',r' + arrayToString(redaundantIndexes)\n",
    "\n",
    "                        saveName = pt.getWeightName(dsName, dataset, batch_size, epochs, numOfLayers, header, dmodel, dfff, dropout, att_dropout, doSkip, doBn, doClsTocken,learning = False, results = True, resultsPath=filteredResultsDir)\n",
    "                        #saveName = pt.getWeightName(dsName, dataset, batch_size, epochs, numOfLayers, header, dmodel, dfff, dropout, att_dropout, doSkip, doBn, learning = False, results = True, resultsPath=filteredResultsDir)\n",
    "\n",
    "                        #print(saveName)\n",
    "                        if os.path.isfile(saveName + '.pkl'):\n",
    "\n",
    "                            res = allRes[saveName]\n",
    "                            if k not in res.keys():\n",
    "                                continue\n",
    "\n",
    "                            #results = helper.load_obj(saveName)\n",
    "\n",
    "                            #res = dict()\n",
    "                            #for index, v in np.ndenumerate(results):\n",
    "                            #    res = v\n",
    "\n",
    "                            \n",
    "                            #print(hypers)\n",
    "                            #print(test_size)\n",
    "                            #name = \n",
    "                            #print(res[k]['t1.0'][name])\n",
    "                            if c == 2:\n",
    "                                v  = res[k]['wrongImportanceMeanPercent']\n",
    "                            else:\n",
    "                                v  = res[k][c]['wrongImportanceMeanPercent']\n",
    "                            #print(v)\n",
    "                            #for i, val in enumerate(v):\n",
    "                                #resultVs.append(val)\n",
    "                                #resultVs.append(val - res[k]['t0.5']['lasa acc'][i])\n",
    "                                #resultVs.append(val > res[k]['t0.5']['lasa acc'][i])\n",
    "                            resultVs.append(v)\n",
    "                            \n",
    "                            if c == 2:\n",
    "                                v  = res[k]['wrongImportancePercent']\n",
    "                            else:\n",
    "                                v  = res[k][c]['wrongImportancePercent']\n",
    "                            \n",
    "                            for i, val in enumerate(v):\n",
    "                                resultVMeans.append(val)                    \n",
    "                        else:\n",
    "                            print('cant find: ' + saveName)\n",
    "            lables= []\n",
    "            for i in range(nrAnds * andStack):\n",
    "                lables.append('AND-'+str(i))\n",
    "            for i in range(nrOrs * orStack):\n",
    "                lables.append('OR-'+str(i))\n",
    "            for i in range(nrxor * xorStack):\n",
    "                lables.append('XOR-'+str(i))\n",
    "            #for i in range(nrEmpty):\n",
    "            #    lables.append('Empty-'+str(i))\n",
    "            if len(resultVs) != 0:\n",
    "                counts = np.nanmean(resultVs, axis=0)[:-1*nrEmpty]\n",
    "                e =  np.nanstd(resultVs, axis=0)[:-1*nrEmpty]\n",
    "            else:\n",
    "                counts = np.zeros(len(lables))\n",
    "                e = np.zeros(len(lables))\n",
    "\n",
    "            ax1.bar(lables, counts, yerr=e, linestyle='None', capsize=10)\n",
    "            ax1.set_ylabel('Percent')\n",
    "            ax1.set_title(k + ' mean wrong ' + topLevelNames[j] + ' ' + str(c))\n",
    "            ax1.tick_params(labelrotation=90)\n",
    "\n",
    "            #plt.show()\n",
    "            \n",
    "            lables= []\n",
    "            for i in range(nrAnds * andStack):\n",
    "                lables.append('AND-'+str(i))\n",
    "            for i in range(nrOrs * orStack):\n",
    "                lables.append('OR-'+str(i))\n",
    "            for i in range(nrxor * xorStack):\n",
    "                lables.append('XOR-'+str(i))\n",
    "            #for i in range(nrEmpty):\n",
    "            #    lables.append('Empty-'+str(i))\n",
    "            if len(resultVMeans) != 0:\n",
    "                counts = np.nanmean(resultVMeans, axis=0)[:-1*nrEmpty]\n",
    "                e =  np.nanstd(resultVMeans, axis=0)[:-1*nrEmpty]\n",
    "            else:\n",
    "                counts = np.zeros(len(lables))\n",
    "                e = np.zeros(len(lables))\n",
    "\n",
    "            ax2.bar(lables, counts, yerr=e, linestyle='None', capsize=10)\n",
    "            ax2.set_ylabel('Percent')\n",
    "            ax2.set_title(k+' avg. wrong ' +topLevelNames[j] + ' ' + str(c))\n",
    "            ax2.tick_params(labelrotation=90)\n",
    "\n",
    "    fig.savefig(folder + k + ' mean all below baseline.png', dpi = 300, bbox_inches = 'tight')\n",
    "    fig2.savefig(folder + k + ' avg all below baseline.png', dpi = 300, bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "    #{0: 0.9642857142857143, 1: 0.5759162303664922},\n",
    "    \n",
    "    #NOTE Mean Importance kann ein ungleichgewicht darstellen, wenn irrelevant eben eine Baseline wäre!\n",
    "    #-> aufpassen was wirklich baseline importance ist! mean kann dur in diesem fall nicht important den mean verzerren!!! \n",
    "    #NOTE Google paper da, könnten die Ecken eben eine Überprüfung sein, hier ist nichts von Bedeutung!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    names = ['generalInformationBelowBaseline','neededInformationBelowBaseline']\n",
    "    saveNames = ['GIB', 'NIB']\n",
    "    for nami, name in enumerate(names):\n",
    "        fig, axs = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True,\n",
    "                                        figsize=(12, 5))\n",
    "\n",
    "\n",
    "        for c in [0,1]:\n",
    "            for j, toplevels in enumerate(topLevelss[1:]):\n",
    "            \n",
    "                ax = axs[c,j]\n",
    "                resultVFs = []\n",
    "                resultVSs = []\n",
    "\n",
    "                kNames = ['LRP-Full', 'LRP-Rollout', 'LRP-Transformer', 'LRP-Transformer CLS', 'IntegratedGradients', 'DeepLift', 'KernelSHAP', 'GuidedGradCam', 'FeaturePermutation', 'Deconvolution','GradCAM++', 'GradCAM']\n",
    "                ks = ['LRP-full', 'LRP-rollout', 'LRP-transformer_attribution', 'LRP-transformer_attribution cls', 'captum-IntegratedGradients', 'captum-DeepLift', 'captum-KernelShap', 'captum-GuidedGradCam', 'captum-FeaturePermutation', 'captum-Deconvolution','PytGradCam-GradCAMPlusPlus', 'PytGradCam-GradCAM']\n",
    "            \n",
    "                for k in ks:\n",
    "                    resultVs = []\n",
    "\n",
    "                    if k in ['performance', 'tree performance', 'treeImportances', 'baseline']:\n",
    "                        continue\n",
    "\n",
    "                    if k == 'LRP-transformer_attribution cls':\n",
    "                        k = 'LRP-transformer_attribution'\n",
    "                        takeCls = True\n",
    "                    else:\n",
    "                        takeCls = False\n",
    "\n",
    "                    for cNr, config in enumerate(allConfigs):\n",
    "                        configName = config.configName\n",
    "                        hyperss = config.hyperss\n",
    "                        hypNames= config.hypNames\n",
    "                        test_sizes = config.test_sizes\n",
    "                        topLevelss = config.topLevelss\n",
    "                        dataset = config.dataset\n",
    "                        symbols = config.symbols\n",
    "                        nrEmpty = config.nrEmpty\n",
    "                        andStack = config.andStack\n",
    "                        orStack = config.orStack\n",
    "                        xorStack = config.xorStack\n",
    "                        nrAnds = config.nrAnds\n",
    "                        nrOrs = config.nrOrs\n",
    "                        nrxor = config.nrxor\n",
    "                        trueIndexes = config.trueIndexes\n",
    "                        orOffSet = config.orOffSet\n",
    "                        xorOffSet = config.xorOffSet\n",
    "                        redaundantIndexes = config.redaundantIndexes\n",
    "                        batch_size = config.batch_size\n",
    "\n",
    "                        for hypers in hyperss:\n",
    "                            if takeCls and not hypers[-1]:\n",
    "                                continue\n",
    "                            elif not takeCls and hypers[-1]:\n",
    "                                continue\n",
    "                            for toplevel in toplevels:\n",
    "                                for test_size in test_sizes:\n",
    "                                    modelType = hypers[0]\n",
    "                                    epochs = hypers[1]\n",
    "                                    dmodel = hypers[2]\n",
    "                                    dfff = hypers[3]\n",
    "                                    doSkip = hypers[4]\n",
    "                                    doBn = hypers[5]\n",
    "                                    header = hypers[6]\n",
    "                                    numOfLayers = hypers[7]\n",
    "                                    dropout = hypers[8]\n",
    "                                    att_dropout = hypers[9]\n",
    "                                    if modelType == 'Transformer':\n",
    "                                        doClsTocken = hypers[10]\n",
    "                                    else:\n",
    "                                        doClsTocken = False\n",
    "\n",
    "                                    dsName = str(dataset) +  ',l' + str(toplevel) +',s' +  str(symbols)+',e' +  str(nrEmpty)+',a' +  str(andStack)+',o' +  str(orStack)+',x' +  str(xorStack)+',na' +  str(nrAnds)+',no' +  str(nrOrs)+',nx' +  str(nrxor)+',i' +  arrayToString(trueIndexes)+',t' +  str(test_size)+',oo' +  str(orOffSet)+',xo' +  str(xorOffSet) +',r' + arrayToString(redaundantIndexes)\n",
    "\n",
    "                                    saveName = pt.getWeightName(dsName, dataset, batch_size, epochs, numOfLayers, header, dmodel, dfff, dropout, att_dropout, doSkip, doBn, doClsTocken,learning = False, results = True, resultsPath=filteredResultsDir)\n",
    "\n",
    "                                    if os.path.isfile(saveName + '.pkl'):\n",
    "\n",
    "                                        res = allRes[saveName]\n",
    "\n",
    "                                        if k not in res.keys():\n",
    "                                            continue\n",
    "\n",
    "                                        v  = res[k][c][name]\n",
    "                                        for i, val in enumerate(v):\n",
    "                                            if res['performance'][i] < accTreshold:\n",
    "                                                continue\n",
    "                                            resultVs.append(np.nanmean(val)*100) \n",
    "                                    else:\n",
    "                                        print('cant find: ' + saveName)\n",
    "                    resultVFs.append(np.nanmean(resultVs))\n",
    "                    resultVSs.append(np.nanstd(resultVs))\n",
    "\n",
    "                lables= kNames\n",
    "                    \n",
    "                counts = resultVFs\n",
    "                counts = np.where(np.isnan(counts),0,counts)\n",
    "                e = resultVSs\n",
    "                e = np.where(np.isnan(e),0,e)\n",
    "\n",
    "\n",
    "                ax.bar(lables, counts, yerr=e, linestyle='None', capsize=4)\n",
    "                ax.set_ylabel('Percent')\n",
    "                ax.set_title(saveNames[nami]+ ' Class ' + str(c) + ' '+ topLevelNames[j+1].upper())\n",
    "                ax.tick_params(labelrotation=90)\n",
    "                ax.set_ylim(bottom=0, top=100)\n",
    "\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(folderGeneral + 'avg'+saveNames[nami]+'All.png', dpi = 300, bbox_inches = 'tight')\n",
    "\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['generalInformationBelowBaseline', 'neededInformationBelowBaseline']\n",
    "saveNames = ['GIB', 'NIB']\n",
    "\n",
    "for cNr, config in enumerate(allConfigs):\n",
    "    configName = config.configName\n",
    "    hyperss = config.hyperss\n",
    "    hypNames= config.hypNames\n",
    "    test_sizes = config.test_sizes\n",
    "    topLevelss = config.topLevelss\n",
    "    dataset = config.dataset\n",
    "    symbols = config.symbols\n",
    "    nrEmpty = config.nrEmpty\n",
    "    andStack = config.andStack\n",
    "    orStack = config.orStack\n",
    "    xorStack = config.xorStack\n",
    "    nrAnds = config.nrAnds\n",
    "    nrOrs = config.nrOrs\n",
    "    nrxor = config.nrxor\n",
    "    trueIndexes = config.trueIndexes\n",
    "    orOffSet = config.orOffSet\n",
    "    xorOffSet = config.xorOffSet\n",
    "    redaundantIndexes = config.redaundantIndexes\n",
    "    batch_size = config.batch_size\n",
    "    for nami, name in enumerate(names):\n",
    "        fig, axs = plt.subplots(nrows=2, ncols=4, sharex=True, sharey=True,\n",
    "                                        figsize=(12, 6))\n",
    "\n",
    "        for c in [0,1]:\n",
    "            for j, toplevels in enumerate(topLevelss):\n",
    "\n",
    "                ax = axs[c,j]\n",
    "                resultVFs = []\n",
    "                resultVSs = []\n",
    "\n",
    "                ks = ['LRP-full', 'LRP-rollout', 'LRP-transformer_attribution', 'captum-IntegratedGradients', 'captum-DeepLift', 'captum-KernelShap', 'captum-GuidedGradCam', 'captum-FeaturePermutation', 'captum-Deconvolution','PytGradCam-GradCAMPlusPlus', 'PytGradCam-GradCAM']\n",
    "                for k in ks:\n",
    "                    resultVs = []\n",
    "\n",
    "                    if k in ['performance', 'tree performance', 'treeImportances', 'baseline']:\n",
    "                        continue\n",
    "\n",
    "                    for hypers in hyperss:\n",
    "                        for toplevel in toplevels:\n",
    "                            for test_size in test_sizes:\n",
    "                                modelType = hypers[0]\n",
    "                                epochs = hypers[1]\n",
    "                                dmodel = hypers[2]\n",
    "                                dfff = hypers[3]\n",
    "                                doSkip = hypers[4]\n",
    "                                doBn = hypers[5]\n",
    "                                header = hypers[6]\n",
    "                                numOfLayers = hypers[7]\n",
    "                                dropout = hypers[8]\n",
    "                                att_dropout = hypers[9]\n",
    "                                if modelType == 'Transformer':\n",
    "                                    doClsTocken = hypers[10]\n",
    "                                else:\n",
    "                                    doClsTocken = False\n",
    "\n",
    "                                dsName = str(dataset) +  ',l' + str(toplevel) +',s' +  str(symbols)+',e' +  str(nrEmpty)+',a' +  str(andStack)+',o' +  str(orStack)+',x' +  str(xorStack)+',na' +  str(nrAnds)+',no' +  str(nrOrs)+',nx' +  str(nrxor)+',i' +  arrayToString(trueIndexes)+',t' +  str(test_size)+',oo' +  str(orOffSet)+',xo' +  str(xorOffSet) +',r' + arrayToString(redaundantIndexes)\n",
    "\n",
    "                                saveName = pt.getWeightName(dsName, dataset, batch_size, epochs, numOfLayers, header, dmodel, dfff, dropout, att_dropout, doSkip, doBn, doClsTocken,learning = False, results = True, resultsPath=filteredResultsDir)\n",
    "\n",
    "                                if os.path.isfile(saveName + '.pkl'):\n",
    "\n",
    "                                    res = allRes[saveName]\n",
    "\n",
    "                                    if k not in res.keys():\n",
    "                                        continue\n",
    "\n",
    "                                    v  = res[k][c][name]\n",
    "                                    for i, val in enumerate(v):\n",
    "                                        if res['performance'][i] < accTreshold:\n",
    "                                                continue\n",
    "                                        resultVs.append(np.nanmean(val) * 100) \n",
    "                                else:\n",
    "                                    print('cant find: ' + saveName)\n",
    "                    resultVFs.append(np.nanmean(resultVs))\n",
    "                    resultVSs.append(np.nanstd(resultVs))\n",
    "\n",
    "                lables= ks\n",
    "                    \n",
    "                counts = resultVFs\n",
    "                counts = np.where(np.isnan(counts),0,counts)\n",
    "                e = resultVSs\n",
    "                e = np.where(np.isnan(e),0,e)\n",
    "\n",
    "\n",
    "                ax.bar(lables, counts, yerr=e, linestyle='None', capsize=4)\n",
    "                ax.set_ylabel('Percent')\n",
    "                ax.set_title(saveNames[nami]+' Class' + str(c) + '\\n'+ topLevelNames[j].upper())\n",
    "                ax.tick_params(labelrotation=90)\n",
    "                ax.set_ylim(bottom=0, top=100)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig.suptitle(config.configName,fontsize=14)\n",
    "        fig.subplots_adjust(top=0.84)\n",
    "        fig.savefig(folderGeneral + saveNames[nami] +' ALL ' + configName+'.png', dpi = 300, bbox_inches = 'tight')\n",
    "\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import seaborn as sns  \n",
    "    colors = sns.color_palette()\n",
    "    colors.append('yellowgreen')\n",
    "    colors.append('darkred')\n",
    "    allReses = dict()\n",
    "    allReses[filteredResultsDir] = allRes\n",
    "    kNames = ['LRP-Full', 'LRP-Rollout', 'LRP-Transformer', 'LRP-Transformer CLS', 'IntegratedGradients', 'DeepLift', 'KernelSHAP', 'GuidedGradCam', 'FeaturePermutation', 'Deconvolution','GradCAM++', 'GradCAM']\n",
    "\n",
    "    ks = ['LRP-full', 'LRP-rollout', 'LRP-transformer_attribution', 'LRP-transformer_attribution cls', 'captum-IntegratedGradients', 'captum-DeepLift', 'captum-KernelShap', 'captum-GuidedGradCam', 'captum-FeaturePermutation', 'captum-Deconvolution','PytGradCam-GradCAMPlusPlus', 'PytGradCam-GradCAM']\n",
    "\n",
    "    names = ['lasa acc', 'lasa red', 'treeScores', 'LogicalAcc', 'LogicalAccDiff', 'LogicalAccStatistics', 'LogicalAccStatisticsDiff']\n",
    "    nameLables = ['Avg. Retrained Model Acc.', 'Avg. Masking Data', 'treeScores', 'Logical Acc.', 'Avg. Logical Acc. Diff.', 'LogicalAccStatistics', 'Avg. Statistical\\n Logical Acc. Diff.'] #'MaxMappingAccDiff' 'MaxMappingAcc\n",
    "    nameLables2 = ['Avg. Retrained Model Acc.', 'Avg. Masking Data', 'treeScores', 'Logical Acc.', 'Avg. Logical Acc. Diff.', 'LogicalAccStatistics', 'Avg. Statistical Logical Acc. Diff.'] #'MaxMappingAccDiff' 'MaxMappingAcc\n",
    "\n",
    "    for c, name in enumerate(names):\n",
    "        fig, axs1 = plt.subplots(nrows=1, ncols=len(topLevelss[1:]), sharex=True, sharey=True,\n",
    "                                        figsize=(10, 4))\n",
    "\n",
    "        plots = []\n",
    "        diff = False\n",
    "        if name[-4:] == 'Diff':\n",
    "            name = name[:-4]\n",
    "            diff = True\n",
    "\n",
    "\n",
    "        DiffresultVs = 0\n",
    "        AllDiffs = 0\n",
    "        lableNames = []\n",
    "        for ki, k in enumerate(ks):\n",
    "            if k is 'baseline' and name not in ['lasa acc', 'MaxMappingAcc']:\n",
    "                continue\n",
    "            if k == 'LRP-transformer_attribution cls':\n",
    "                k = 'LRP-transformer_attribution'\n",
    "                takeCls = True\n",
    "            else:\n",
    "                takeCls = False\n",
    "\n",
    "            for j, toplevels in enumerate(topLevelss[1:]):\n",
    "                ax1 = axs1[j]\n",
    "                AllDiffs = [[],[],[],[]]\n",
    "                DiffresultVs = [[],[],[],[]]\n",
    "                resultVs = [[],[],[],[]]\n",
    "\n",
    "\n",
    "                for conI, config in enumerate(allConfigs):\n",
    "                    hyperss = config.hyperss\n",
    "                    hypNames= config.hypNames\n",
    "                    test_sizes = config.test_sizes\n",
    "\n",
    "                    dataset = config.dataset\n",
    "                    symbols = config.symbols\n",
    "                    nrEmpty = config.nrEmpty\n",
    "                    andStack = config.andStack\n",
    "                    orStack = config.orStack\n",
    "                    xorStack = config.xorStack\n",
    "                    nrAnds = config.nrAnds\n",
    "                    nrOrs = config.nrOrs\n",
    "                    nrxor = config.nrxor\n",
    "                    trueIndexes = config.trueIndexes\n",
    "                    orOffSet = config.orOffSet\n",
    "                    xorOffSet = config.xorOffSet\n",
    "                    redaundantIndexes = config.redaundantIndexes\n",
    "                    batch_size = config.batch_size\n",
    "\n",
    "                    \n",
    "                    if k in ['performance', 'tree performance', 'treeImportances']:\n",
    "                        continue\n",
    "\n",
    "                    for hypers in hyperss:\n",
    "                        if takeCls and not hypers[-1]:\n",
    "                            continue\n",
    "                        elif not takeCls and hypers[-1]:\n",
    "                            continue\n",
    "                        for toplevel in toplevels:\n",
    "                            for test_size in test_sizes:\n",
    "                                modelType = hypers[0]\n",
    "                                epochs = hypers[1]\n",
    "                                dmodel = hypers[2]\n",
    "                                dfff = hypers[3]\n",
    "                                doSkip = hypers[4]\n",
    "                                doBn = hypers[5]\n",
    "                                header = hypers[6]\n",
    "                                numOfLayers = hypers[7]\n",
    "                                dropout = hypers[8]\n",
    "                                att_dropout = hypers[9]\n",
    "                                if modelType == 'Transformer':\n",
    "                                    doClsTocken = hypers[10]\n",
    "                                else:\n",
    "                                    doClsTocken = False\n",
    "\n",
    "                                dsName = str(dataset) +  ',l' + str(toplevel) +',s' +  str(symbols)+',e' +  str(nrEmpty)+',a' +  str(andStack)+',o' +  str(orStack)+',x' +  str(xorStack)+',na' +  str(nrAnds)+',no' +  str(nrOrs)+',nx' +  str(nrxor)+',i' +  arrayToString(trueIndexes)+',t' +  str(test_size)+',oo' +  str(orOffSet)+',xo' +  str(xorOffSet) +',r' + arrayToString(redaundantIndexes)\n",
    "                                \n",
    "                                for rFolder in ['filteredResultsDCA','filteredResultsBaselineDCA']:\n",
    "                                    if rFolder not in allReses.keys():\n",
    "                                        allReses[rFolder] = dict()\n",
    "                                        \n",
    "                                    saveName = pt.getWeightName(dsName, dataset, batch_size, epochs, numOfLayers, header, dmodel, dfff, dropout, att_dropout, doSkip, doBn, doClsTocken, learning = False, results = True, resultsPath=rFolder)\n",
    "\n",
    "                                    if os.path.isfile(saveName + '.pkl'):\n",
    "                                        if saveName in allReses[filteredResultsDir].keys():\n",
    "                                            res = allReses[filteredResultsDir][saveName]\n",
    "                                        else:\n",
    "                                            results = helper.load_obj(saveName)\n",
    "\n",
    "                                            res = dict()\n",
    "                                            for index, vv in np.ndenumerate(results):\n",
    "                                                res = vv\n",
    "\n",
    "                                            allReses[filteredResultsDir][saveName] = res\n",
    "                                    else:\n",
    "                                        print('NOT FOUND: ' + saveName)\n",
    "\n",
    "                                    if k not in res.keys():\n",
    "                                        continue\n",
    "\n",
    "\n",
    "                                    for i, t in enumerate(['tbaseline','t1.0', 't0.8', 't0.5']):\n",
    "                                        if t not in res[k].keys(): \n",
    "                                            continue\n",
    "                                        if k is 'baseline':\n",
    "                                            v  = res[k]\n",
    "                                            if res['baseline'][0] < 0.5:\n",
    "                                                v = [1-res['baseline'][0]]\n",
    "                                        else:\n",
    "                                            v  = res[k][t][name]\n",
    "\n",
    "                                        if diff:\n",
    "                                            for foldI, val in enumerate(v):\n",
    "                                                if res['performance'][foldI] < accTreshold:\n",
    "                                                    continue\n",
    "                                                resultVs[i].append(100* (res[k][t]['lasa acc'][foldI] - np.mean([val])))\n",
    "                                                \n",
    "                                        else:\n",
    "                                            for foldI, val in enumerate(v):\n",
    "                                                if res['performance'][foldI] < accTreshold:\n",
    "                                                    continue\n",
    "                                                resultVs[i].append(np.mean([val])*100)\n",
    "                                                DiffresultVs[i].append((1 - res[k][0]['neededInformationBelowBaseline'][foldI]*res['baseline'][0] - res[k][1]['neededInformationBelowBaseline'][foldI]*(1-res['baseline'][0])) >= np.mean([val]))\n",
    "                                                AllDiffs[i].append(True)\n",
    "\n",
    "                lables= []\n",
    "                for i in range(1):\n",
    "                    lables.append('tbaseline')\n",
    "                for i in range(1):\n",
    "                    lables.append('t1.0')\n",
    "                for i in range(1):\n",
    "                    lables.append('t0.8')\n",
    "                for i in range(1):\n",
    "                    lables.append('t0.5')                            \n",
    "\n",
    "                counts = [[],[],[],[]]\n",
    "                e = [[],[],[],[]] \n",
    "                for v in range(len(resultVs)):\n",
    "                    counts[v] = np.nanmean(np.array(resultVs[v]))\n",
    "                    e[v] = np.nanstd(np.array(resultVs[v]))\n",
    "\n",
    "\n",
    "\n",
    "                lableName = kNames[ki]\n",
    "                if lableName not in lableNames:\n",
    "                    lableNames.append(lableName)\n",
    "\n",
    "                p = ax1.plot(lables, counts, 's-', label=lableName, color=colors[ki])\n",
    "\n",
    "                plots.append(p)\n",
    "                ax1.set_ylabel('Percent')\n",
    "\n",
    "                ax1.set_title(nameLables[c] +' ' + topLevelNames[j+1].upper())\n",
    "\n",
    "            \n",
    "            \n",
    "        fig.legend(plots, labels=lableNames, \n",
    "                loc=\"upper right\", bbox_to_anchor=(1.11, 0.80)) \n",
    "        fig.savefig(folderGeneral + nameLables2[c] + '-Performanceavg.png', dpi = 300, bbox_inches = 'tight')\n",
    "\n",
    "        plt.show()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import seaborn as sns  \n",
    "    colors = sns.color_palette()\n",
    "    colors.append('yellowgreen')\n",
    "    colors.append('darkred')\n",
    "    allReses = dict()\n",
    "    allReses[filteredResultsDir] = allRes\n",
    "    kNames = ['LRP-Full', 'LRP-Rollout', 'LRP-Transformer', 'LRP-Transformer CLS', 'IntegratedGradients', 'DeepLift', 'KernelSHAP', 'GuidedGradCam', 'FeaturePermutation', 'Deconvolution','GradCAM++', 'GradCAM']\n",
    "\n",
    "    ks = ['LRP-full', 'LRP-rollout', 'LRP-transformer_attribution', 'LRP-transformer_attribution cls', 'captum-IntegratedGradients', 'captum-DeepLift', 'captum-KernelShap', 'captum-GuidedGradCam', 'captum-FeaturePermutation', 'captum-Deconvolution','PytGradCam-GradCAMPlusPlus', 'PytGradCam-GradCAM']\n",
    "\n",
    "    names = ['lasa acc', 'lasa red', 'treeScores', 'LogicalAcc', 'LogicalAccDiff', 'LogicalAccStatistics', 'LogicalAccStatisticsDiff']\n",
    "    nameLables = ['std. Retrained Model Acc.', 'std. Masking Data', 'std. treeScores', 'std. Logical Acc.', 'std. Logical Acc. Diff.', 'std. LogicalAccStatistics', 'std. Statistical\\n Logical Acc. Diff.'] #'MaxMappingAccDiff' 'MaxMappingAcc\n",
    "    nameLables2 = ['std. Retrained Model Acc.', 'std. Masking Data', 'std. treeScores', 'std. Logical Acc.', 'std. Logical Acc. Diff.', 'std. LogicalAccStatistics', 'std. Statistical Logical Acc. Diff.'] #'MaxMappingAccDiff' 'MaxMappingAcc\n",
    "\n",
    "    for c, name in enumerate(names):\n",
    "        fig, axs1 = plt.subplots(nrows=1, ncols=len(topLevelss[1:]), sharex=True, sharey=True,\n",
    "                                        figsize=(10, 4))\n",
    "\n",
    "        plots = []\n",
    "        diff = False\n",
    "        if name[-4:] == 'Diff':\n",
    "            name = name[:-4]\n",
    "            diff = True\n",
    "\n",
    "\n",
    "        DiffresultVs = 0\n",
    "        AllDiffs = 0\n",
    "        lableNames = []\n",
    "        for ki, k in enumerate(ks):\n",
    "            if k is 'baseline' and name not in ['lasa acc', 'MaxMappingAcc']:\n",
    "                continue\n",
    "            if k == 'LRP-transformer_attribution cls':\n",
    "                k = 'LRP-transformer_attribution'\n",
    "                takeCls = True\n",
    "            else:\n",
    "                takeCls = False\n",
    "\n",
    "            for j, toplevels in enumerate(topLevelss[1:]):\n",
    "                ax1 = axs1[j]\n",
    "                AllDiffs = [[],[],[],[]]\n",
    "                DiffresultVs = [[],[],[],[]]\n",
    "                resultVs = [[],[],[],[]]\n",
    "\n",
    "\n",
    "                for conI, config in enumerate(allConfigs):\n",
    "                    hyperss = config.hyperss\n",
    "                    hypNames= config.hypNames\n",
    "                    test_sizes = config.test_sizes\n",
    "\n",
    "                    dataset = config.dataset\n",
    "                    symbols = config.symbols\n",
    "                    nrEmpty = config.nrEmpty\n",
    "                    andStack = config.andStack\n",
    "                    orStack = config.orStack\n",
    "                    xorStack = config.xorStack\n",
    "                    nrAnds = config.nrAnds\n",
    "                    nrOrs = config.nrOrs\n",
    "                    nrxor = config.nrxor\n",
    "                    trueIndexes = config.trueIndexes\n",
    "                    orOffSet = config.orOffSet\n",
    "                    xorOffSet = config.xorOffSet\n",
    "                    redaundantIndexes = config.redaundantIndexes\n",
    "                    batch_size = config.batch_size\n",
    "\n",
    "                    \n",
    "                    if k in ['performance', 'tree performance', 'treeImportances']:\n",
    "                        continue\n",
    "\n",
    "                    for hypers in hyperss:\n",
    "                        if takeCls and not hypers[-1]:\n",
    "                            continue\n",
    "                        elif not takeCls and hypers[-1]:\n",
    "                            continue\n",
    "                        for toplevel in toplevels:\n",
    "                            for test_size in test_sizes:\n",
    "                                modelType = hypers[0]\n",
    "                                epochs = hypers[1]\n",
    "                                dmodel = hypers[2]\n",
    "                                dfff = hypers[3]\n",
    "                                doSkip = hypers[4]\n",
    "                                doBn = hypers[5]\n",
    "                                header = hypers[6]\n",
    "                                numOfLayers = hypers[7]\n",
    "                                dropout = hypers[8]\n",
    "                                att_dropout = hypers[9]\n",
    "                                if modelType == 'Transformer':\n",
    "                                    doClsTocken = hypers[10]\n",
    "                                else:\n",
    "                                    doClsTocken = False\n",
    "\n",
    "                                dsName = str(dataset) +  ',l' + str(toplevel) +',s' +  str(symbols)+',e' +  str(nrEmpty)+',a' +  str(andStack)+',o' +  str(orStack)+',x' +  str(xorStack)+',na' +  str(nrAnds)+',no' +  str(nrOrs)+',nx' +  str(nrxor)+',i' +  arrayToString(trueIndexes)+',t' +  str(test_size)+',oo' +  str(orOffSet)+',xo' +  str(xorOffSet) +',r' + arrayToString(redaundantIndexes)\n",
    "                                \n",
    "                                for rFolder in ['filteredResultsDCA','filteredResultsBaselineDCA']:\n",
    "                                    if rFolder not in allReses.keys():\n",
    "                                        allReses[rFolder] = dict()\n",
    "                                        \n",
    "                                    saveName = pt.getWeightName(dsName, dataset, batch_size, epochs, numOfLayers, header, dmodel, dfff, dropout, att_dropout, doSkip, doBn, doClsTocken, learning = False, results = True, resultsPath=rFolder)\n",
    "\n",
    "                                    if os.path.isfile(saveName + '.pkl'):\n",
    "                                        if saveName in allReses[filteredResultsDir].keys():\n",
    "                                            res = allReses[filteredResultsDir][saveName]\n",
    "                                        else:\n",
    "                                            results = helper.load_obj(saveName)\n",
    "\n",
    "                                            res = dict()\n",
    "                                            for index, vv in np.ndenumerate(results):\n",
    "                                                res = vv\n",
    "\n",
    "                                            allReses[filteredResultsDir][saveName] = res\n",
    "                                    else:\n",
    "                                        print('NOT FOUND: ' + saveName)\n",
    "\n",
    "                                    if k not in res.keys():\n",
    "                                        continue\n",
    "\n",
    "\n",
    "                                    for i, t in enumerate(['tbaseline','t1.0', 't0.8', 't0.5']):\n",
    "                                        if t not in res[k].keys(): \n",
    "                                            continue\n",
    "                                        if k is 'baseline':\n",
    "                                            v  = res[k]\n",
    "                                            if res['baseline'][0] < 0.5:\n",
    "                                                v = [1-res['baseline'][0]]\n",
    "                                        else:\n",
    "                                            v  = res[k][t][name]\n",
    "\n",
    "                                        if diff:\n",
    "                                            for foldI, val in enumerate(v):\n",
    "                                                if res['performance'][foldI] < accTreshold:\n",
    "                                                    continue\n",
    "                                                resultVs[i].append(100* (res[k][t]['lasa acc'][foldI] - np.mean([val])))\n",
    "                                                \n",
    "                                        else:\n",
    "                                            for foldI, val in enumerate(v):\n",
    "                                                if res['performance'][foldI] < accTreshold:\n",
    "                                                    continue\n",
    "                                                resultVs[i].append(np.mean([val])*100)\n",
    "                                                DiffresultVs[i].append((1 - res[k][0]['neededInformationBelowBaseline'][foldI]*res['baseline'][0] - res[k][1]['neededInformationBelowBaseline'][foldI]*(1-res['baseline'][0])) >= np.mean([val]))\n",
    "                                                AllDiffs[i].append(True)\n",
    "\n",
    "                lables= []\n",
    "                for i in range(1):\n",
    "                    lables.append('tbaseline')\n",
    "                for i in range(1):\n",
    "                    lables.append('t1.0')\n",
    "                for i in range(1):\n",
    "                    lables.append('t0.8')\n",
    "                for i in range(1):\n",
    "                    lables.append('t0.5')                            \n",
    "\n",
    "                counts = [[],[],[],[]]\n",
    "                e = [[],[],[],[]] \n",
    "                for v in range(len(resultVs)):\n",
    "                    counts[v] = np.nanmean(np.array(resultVs[v]))\n",
    "                    e[v] = np.nanstd(np.array(resultVs[v]))\n",
    "\n",
    "\n",
    "\n",
    "                lableName = kNames[ki]\n",
    "                if lableName not in lableNames:\n",
    "                    lableNames.append(lableName)\n",
    "\n",
    "                p = ax1.plot(lables, e, 's-', label=lableName, color=colors[ki])\n",
    "\n",
    "                plots.append(p)\n",
    "                ax1.set_ylabel('Percent')\n",
    "\n",
    "                ax1.set_title(nameLables[c] +' ' + topLevelNames[j+1].upper())\n",
    "\n",
    "            \n",
    "            \n",
    "        fig.legend(plots, labels=lableNames, \n",
    "                loc=\"upper right\", bbox_to_anchor=(1.11, 0.80)) \n",
    "        fig.savefig(folderGeneral + nameLables2[c] + '-avgSTD.png', dpi = 300, bbox_inches = 'tight')\n",
    "\n",
    "        plt.show()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allReses = dict()\n",
    "allReses[filteredResultsDir] = allRes\n",
    "\n",
    "kNames = ['LRP-Full', 'LRP-Rollout', 'LRP-Transformer', 'LRP-Transformer CLS', 'IntegratedGradients', 'DeepLift', 'KernelSHAP', 'GuidedGradCam', 'FeaturePermutation', 'Deconvolution','GradCAM++', 'GradCAM']\n",
    "\n",
    "ks = ['LRP-full', 'LRP-rollout', 'LRP-transformer_attribution', 'LRP-transformer_attribution cls', 'captum-IntegratedGradients', 'captum-DeepLift', 'captum-KernelShap', 'captum-GuidedGradCam', 'captum-FeaturePermutation', 'captum-Deconvolution','PytGradCam-GradCAMPlusPlus', 'PytGradCam-GradCAM']\n",
    "tables = ['DoubleAssigmentFullPercent']\n",
    "\n",
    "for tab in tables:\n",
    "    \n",
    "    fig, axs1 = plt.subplots(nrows=3, ncols=len(topLevelss[1:]), sharex=True, sharey=False,\n",
    "                                        figsize=(16, 8))\n",
    "    plots = []\n",
    "    lables = []\n",
    "    Allcounts = [[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "    for i, t in enumerate(['t1.0', 't0.8', 't0.5']):\n",
    "        \n",
    "        for logic in ['and0']:        \n",
    "            \n",
    "            #NOTE control plots here\n",
    "            for testFac in [0,1]: # 0 = all; 1 = splitted data\n",
    "                for accT in [0,1]: # 0= no acc threshold; 1= only 100% data\n",
    "                    #NOTE comment out if all 4 graphs should be plotted\n",
    "                    #if testFac != accT:\n",
    "                    #    continue\n",
    "                    for conI, config in enumerate(allConfigs):\n",
    "                        testFacNames = ['Not Split Test','Split Test Set']\n",
    "                        accTNames = ['Acc>0','Acc=100']\n",
    "                        hyperss = config.hyperss\n",
    "                        hypNames= config.hypNames\n",
    "                        test_sizes = np.array(config.test_sizes) * testFac\n",
    "                        topLevelss = config.topLevelss\n",
    "                        toplevels = config.toplevels\n",
    "                        dataset = config.dataset\n",
    "                        symbols = config.symbols\n",
    "                        nrEmpty = config.nrEmpty\n",
    "                        andStack = config.andStack\n",
    "                        orStack = config.orStack\n",
    "                        xorStack = config.xorStack\n",
    "                        nrAnds = config.nrAnds\n",
    "                        nrOrs = config.nrOrs\n",
    "                        nrxor = config.nrxor\n",
    "                        trueIndexes = config.trueIndexes\n",
    "                        orOffSet = config.orOffSet\n",
    "                        xorOffSet = config.xorOffSet\n",
    "                        redaundantIndexes = config.redaundantIndexes\n",
    "                        batch_size = config.batch_size\n",
    "                        \n",
    "                        stackNumbers = andStack * nrAnds + orStack * nrOrs + xorStack * nrxor\n",
    "                        datasetSize = symbols**(andStack * nrAnds + orStack * nrOrs + xorStack * nrxor + nrEmpty)\n",
    "                        \n",
    "                        for j, toplevels in enumerate(topLevelss[1:]):\n",
    "\n",
    "                            ax1 = axs1[conI,j]\n",
    "                            resultVs = [[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "\n",
    "                            for c, k in enumerate(ks):\n",
    "                                if k in ['performance', 'tree performance', 'treeImportances']:\n",
    "                                    continue\n",
    "\n",
    "                                if k == 'LRP-transformer_attribution cls':\n",
    "                                    k = 'LRP-transformer_attribution'\n",
    "                                    takeCls = True\n",
    "                                else:\n",
    "                                    takeCls = False\n",
    "\n",
    "                                for hypers in hyperss:\n",
    "                                    if takeCls and not hypers[-1]:\n",
    "                                        continue\n",
    "                                    elif not takeCls and hypers[-1]:\n",
    "                                        continue\n",
    "                                    for toplevel in toplevels:\n",
    "                                        for test_size in test_sizes:\n",
    "                                            if test_size == 0:\n",
    "                                                test_size = int(0)\n",
    "                                            modelType = hypers[0]\n",
    "                                            epochs = hypers[1]\n",
    "                                            dmodel = hypers[2]\n",
    "                                            dfff = hypers[3]\n",
    "                                            doSkip = hypers[4]\n",
    "                                            doBn = hypers[5]\n",
    "                                            header = hypers[6]\n",
    "                                            numOfLayers = hypers[7]\n",
    "                                            dropout = hypers[8]\n",
    "                                            att_dropout = hypers[9]\n",
    "                                            if modelType == 'Transformer':\n",
    "                                                doClsTocken = hypers[10]\n",
    "                                            else:\n",
    "                                                doClsTocken = False\n",
    "\n",
    "                                            dsName = str(dataset) +  ',l' + str(toplevel) +',s' +  str(symbols)+',e' +  str(nrEmpty)+',a' +  str(andStack)+',o' +  str(orStack)+',x' +  str(xorStack)+',na' +  str(nrAnds)+',no' +  str(nrOrs)+',nx' +  str(nrxor)+',i' +  arrayToString(trueIndexes)+',t' +  str(test_size)+',oo' +  str(orOffSet)+',xo' +  str(xorOffSet) +',r' + arrayToString(redaundantIndexes)\n",
    "\n",
    "                                            for rFolder in ['filteredResultsDCA','filteredResultsBaselineDCA']:\n",
    "                                                if rFolder not in allReses.keys():\n",
    "                                                    allReses[rFolder] = dict()\n",
    "\n",
    "                                                saveName = pt.getWeightName(dsName, dataset, batch_size, epochs, numOfLayers, header, dmodel, dfff, dropout, att_dropout, doSkip, doBn, doClsTocken, learning = False, results = True, resultsPath=rFolder)\n",
    "\n",
    "                                                if os.path.isfile(saveName + '.pkl'):\n",
    "                                                    if saveName in allReses[filteredResultsDir].keys():\n",
    "                                                        res = allReses[filteredResultsDir][saveName]\n",
    "                                                    else:\n",
    "                                                        results = helper.load_obj(saveName)\n",
    "\n",
    "                                                        res = dict()\n",
    "                                                        for index, vv in np.ndenumerate(results):\n",
    "                                                            res = vv\n",
    "\n",
    "                                                        allReses[filteredResultsDir][saveName] = res\n",
    "                                                else:\n",
    "                                                    print('NOT FOUND: ' + saveName)\n",
    "                                                if k not in res.keys():\n",
    "                                                    continue\n",
    "                                                if t not in res[k].keys(): \n",
    "                                                    continue\n",
    "                                               \n",
    "\n",
    "                                                v = res[k][t][tab][logic]\n",
    "                                                for valII, val in enumerate(v):\n",
    "                                                    if val == -1:\n",
    "                                                        continue\n",
    "                                                    if res['performance'][valII] < accT:\n",
    "                                                        continue\n",
    "                                                    resultVs[c].append(val*100)\n",
    "\n",
    "\n",
    "                            counts = [[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "                            e = [[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "                            rCount = [[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "                            for vu in range(len(resultVs)):\n",
    "                                counts[vu] = np.nanmean(np.array(resultVs[vu]))\n",
    "                                e[vu] = np.nanstd(np.array(resultVs[vu]))\n",
    "                                rCount[vu] = len(np.array(resultVs[vu]))\n",
    "                                for rv in resultVs[vu]:\n",
    "                                    Allcounts[vu].append(rv)\n",
    "\n",
    "                            colors = ['tab:blue', 'orange','g', 'r']\n",
    "                            colorN = colors[i]\n",
    "                            lableName = t + ' ' + testFacNames[testFac] + ' ' +accTNames[accT]\n",
    "                            if lableName not in lables:\n",
    "                                lables.append(lableName)\n",
    "                            \n",
    "                            if testFac == 1 and accT == 1:\n",
    "                                p = ax1.errorbar(kNames, counts, marker='^', yerr=e,label=lableName, color=colorN, linestyle=\"none\")\n",
    "                            elif testFac == 1 and accT == 0:\n",
    "                                p = ax1.errorbar(kNames, counts, marker='v', yerr=e,label=lableName, color=colorN, linestyle=\"none\")\n",
    "                            elif testFac == 0 and accT == 1:\n",
    "                                p = ax1.errorbar(kNames, counts, marker='<', yerr=e,label=lableName, color=colorN, linestyle=\"none\")\n",
    "                            elif testFac == 0 and accT == 0:\n",
    "                                p = ax1.errorbar(kNames, counts, marker='>', yerr=e,label=lableName, color=colorN, linestyle=\"none\")\n",
    "\n",
    "                            #print(resultVs)\n",
    "                            #print(counts)\n",
    "                            #print(e)\n",
    "                            #print(rCount)\n",
    "                            #print('----'+config.configName + ' ' + lableName + ' ' + toplevels[0] + ' ' + config.configName)\n",
    "                            plots.append(p)\n",
    "                            ax1.tick_params(labelrotation=90)\n",
    "                            ax1.set_ylim(bottom=0)\n",
    "                            if j == 0 and conI == 1:\n",
    "                                ax1.set_ylabel('Full-DCA')\n",
    "                            if conI == 0:\n",
    "                                ax1.set_title(topLevelNames[j+1].upper() + '\\n' + config.configName)\n",
    "                            else:\n",
    "                                ax1.set_title(config.configName)\n",
    "\n",
    "    fig.legend(plots, labels=lables, \n",
    "            loc=\"upper right\", bbox_to_anchor=(1.05, 0.75)) \n",
    "\n",
    "    fig.savefig(folderGeneral +'DoubleAssignmentTruthTableAll.png', dpi = 300, bbox_inches = 'tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    allReses = dict()\n",
    "    allReses[filteredResultsDir] = allRes\n",
    "\n",
    "    kNames = ['LRP-Full', 'LRP-Rollout', 'LRP-Transformer', 'LRP-Transformer CLS', 'IntegratedGradients', 'DeepLift', 'KernelSHAP', 'GuidedGradCam', 'FeaturePermutation', 'Deconvolution','GradCAM++', 'GradCAM']\n",
    "\n",
    "    ks = ['LRP-full', 'LRP-rollout', 'LRP-transformer_attribution', 'LRP-transformer_attribution cls', 'captum-IntegratedGradients', 'captum-DeepLift', 'captum-KernelShap', 'captum-GuidedGradCam', 'captum-FeaturePermutation', 'captum-Deconvolution','PytGradCam-GradCAMPlusPlus', 'PytGradCam-GradCAM']\n",
    "    tables = ['DoubleAssigmentTruthTableMinPercent']\n",
    "\n",
    "\n",
    "\n",
    "    for tab in tables:\n",
    "        \n",
    "        fig, axs1 = plt.subplots(nrows=3, ncols=len(topLevelss[1:]), sharex=True, sharey=False,\n",
    "                                            figsize=(16, 8))\n",
    "        plots = []\n",
    "        lables = []\n",
    "        fullMincounts = [[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "        for i, t in enumerate(['t1.0', 't0.8', 't0.5', 'tbaseline']):\n",
    "            \n",
    "           \n",
    "            #NOTE control plots here\n",
    "            for testFac in [0,1]: # 0 = all; 1 = splitted data\n",
    "                for accT in [0,1]: # 0= no acc threshold; 1= only 100% data\n",
    "                    #NOTE comment in or out to control plots\n",
    "                    #if testFac != accT:\n",
    "                    #    continue\n",
    "                    for conI, config in enumerate(allConfigs):\n",
    "                        testFacNames = ['Not Split Test','Split Test Set']\n",
    "                        accTNames = ['Acc>0','Acc=100']\n",
    "                        hyperss = config.hyperss\n",
    "                        hypNames= config.hypNames\n",
    "                        test_sizes = np.array(config.test_sizes) * testFac\n",
    "                        topLevelss = config.topLevelss\n",
    "                        toplevels = config.toplevels\n",
    "                        dataset = config.dataset\n",
    "                        symbols = config.symbols\n",
    "                        nrEmpty = config.nrEmpty\n",
    "                        andStack = config.andStack\n",
    "                        orStack = config.orStack\n",
    "                        xorStack = config.xorStack\n",
    "                        nrAnds = config.nrAnds\n",
    "                        nrOrs = config.nrOrs\n",
    "                        nrxor = config.nrxor\n",
    "                        trueIndexes = config.trueIndexes\n",
    "                        orOffSet = config.orOffSet\n",
    "                        xorOffSet = config.xorOffSet\n",
    "                        redaundantIndexes = config.redaundantIndexes\n",
    "                        batch_size = config.batch_size\n",
    "                        \n",
    "                        stackNumbers = andStack * nrAnds + orStack * nrOrs + xorStack * nrxor\n",
    "                        datasetSize = symbols**(andStack * nrAnds + orStack * nrOrs + xorStack * nrxor + nrEmpty)\n",
    "                    \n",
    "                        for j, toplevels in enumerate(topLevelss[1:]):\n",
    "\n",
    "                            ax1 = axs1[conI,j]\n",
    "                            resultVs = [[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "\n",
    "                            for logic in ['and0', 'or0', 'xor0']:  \n",
    "\n",
    "                                for c, k in enumerate(ks):\n",
    "                                    if k in ['performance', 'tree performance', 'treeImportances']:\n",
    "                                        continue\n",
    "\n",
    "                                    if k == 'LRP-transformer_attribution cls':\n",
    "                                        k = 'LRP-transformer_attribution'\n",
    "                                        takeCls = True\n",
    "                                    else:\n",
    "                                        takeCls = False\n",
    "\n",
    "                                    for hypers in hyperss:\n",
    "                                        if takeCls and not hypers[-1]:\n",
    "                                            continue\n",
    "                                        elif not takeCls and hypers[-1]:\n",
    "                                            continue\n",
    "                                        for toplevel in toplevels:\n",
    "                                            for test_size in test_sizes:\n",
    "                                                if test_size == 0:\n",
    "                                                    test_size = int(0)\n",
    "                                                modelType = hypers[0]\n",
    "                                                epochs = hypers[1]\n",
    "                                                dmodel = hypers[2]\n",
    "                                                dfff = hypers[3]\n",
    "                                                doSkip = hypers[4]\n",
    "                                                doBn = hypers[5]\n",
    "                                                header = hypers[6]\n",
    "                                                numOfLayers = hypers[7]\n",
    "                                                dropout = hypers[8]\n",
    "                                                att_dropout = hypers[9]\n",
    "                                                if modelType == 'Transformer':\n",
    "                                                    doClsTocken = hypers[10]\n",
    "                                                else:\n",
    "                                                    doClsTocken = False\n",
    "\n",
    "                                                dsName = str(dataset) +  ',l' + str(toplevel) +',s' +  str(symbols)+',e' +  str(nrEmpty)+',a' +  str(andStack)+',o' +  str(orStack)+',x' +  str(xorStack)+',na' +  str(nrAnds)+',no' +  str(nrOrs)+',nx' +  str(nrxor)+',i' +  arrayToString(trueIndexes)+',t' +  str(test_size)+',oo' +  str(orOffSet)+',xo' +  str(xorOffSet) +',r' + arrayToString(redaundantIndexes)\n",
    "\n",
    "                                                for rFolder in ['filteredResultsDCA','filteredResultsBaselineDCA']:\n",
    "                                                    if rFolder not in allReses.keys():\n",
    "                                                        allReses[rFolder] = dict()\n",
    "\n",
    "                                                    saveName = pt.getWeightName(dsName, dataset, batch_size, epochs, numOfLayers, header, dmodel, dfff, dropout, att_dropout, doSkip, doBn, doClsTocken, learning = False, results = True, resultsPath=rFolder)\n",
    "\n",
    "                                                    if os.path.isfile(saveName + '.pkl'):\n",
    "                                                        if saveName in allReses[filteredResultsDir].keys():\n",
    "                                                            res = allReses[filteredResultsDir][saveName]\n",
    "                                                        else:\n",
    "                                                            results = helper.load_obj(saveName)\n",
    "\n",
    "                                                            res = dict()\n",
    "                                                            for index, vv in np.ndenumerate(results):\n",
    "                                                                res = vv\n",
    "\n",
    "                                                            allReses[filteredResultsDir][saveName] = res\n",
    "                                                    else:\n",
    "                                                        print('NOT FOUND: ' + saveName)\n",
    "                                                    if k not in res.keys():\n",
    "                                                        continue\n",
    "                                                    if t not in res[k].keys(): \n",
    "                                                        continue\n",
    "\n",
    "                                                    v = res[k][t][tab][logic]\n",
    "                                                    for valII, val in enumerate(v):\n",
    "                                                        if val == -1:\n",
    "                                                            continue\n",
    "                                                        if res['performance'][valII] < accT:\n",
    "                                                            continue\n",
    "                                                        dFactor = datasetSize * test_size\n",
    "                                                        if dFactor == 0:\n",
    "                                                            dFactor = datasetSize\n",
    "\n",
    "                                                        resultVs[c].append(val*100)\n",
    "\n",
    "                            counts = [[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "                            e = [[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "                            for vu in range(len(resultVs)):\n",
    "                                counts[vu] = np.nanmean(np.array(resultVs[vu]))\n",
    "                                e[vu] = np.nanstd(np.array(resultVs[vu]))\n",
    "                                for resultV in resultVs[vu]:\n",
    "                                    fullMincounts[vu].append(resultV)\n",
    "\n",
    "                            colors = ['tab:blue', 'orange','g', 'tab:red']\n",
    "                            colorN =colors[i]\n",
    "                            lableName = t + ' ' + testFacNames[testFac] + ' ' +accTNames[accT]\n",
    "                            if lableName not in lables:\n",
    "                                lables.append(lableName)\n",
    "                            if testFac == 1 and accT == 1:\n",
    "                                p = ax1.errorbar(kNames, counts, marker='^', yerr=e,label=lableName, color=colorN, linestyle=\"none\")\n",
    "                            elif testFac == 1 and accT == 0:\n",
    "                                p = ax1.errorbar(kNames, counts, marker='v', yerr=e,label=lableName, color=colorN, linestyle=\"none\")\n",
    "                            elif testFac == 0 and accT == 1:\n",
    "                                p = ax1.errorbar(kNames, counts, marker='<', yerr=e,label=lableName, color=colorN, linestyle=\"none\")\n",
    "                            elif testFac == 0 and accT == 0:\n",
    "                                p = ax1.errorbar(kNames, counts, marker='>', yerr=e,label=lableName, color=colorN, linestyle=\"none\")\n",
    "\n",
    "                            #print(resultVs)\n",
    "                            #print(counts)\n",
    "                            #print(e)\n",
    "                            #print('----'+config.configName)\n",
    "                            plots.append(p)\n",
    "                            ax1.tick_params(labelrotation=90)\n",
    "                            ax1.set_ylim(bottom=0)\n",
    "                \n",
    "                            if j == 0 and conI==1:\n",
    "                                ax1.set_ylabel('Minimal-DCA')\n",
    "                            if conI == 0:\n",
    "                                ax1.set_title(topLevelNames[j+1].upper() +'\\n' + config.configName)\n",
    "                            else:\n",
    "                                ax1.set_title(config.configName)\n",
    "\n",
    "        fig.legend(plots, labels=lables, \n",
    "                loc=\"upper right\", bbox_to_anchor=(1.07, 0.75)) \n",
    "\n",
    "        fig.savefig(folderGeneral +'DoubleAssignmentTruthTableMinOverConfigsAll.png', dpi = 300, bbox_inches = 'tight')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Based on the last 2 plots!\n",
    "\n",
    "counts = [[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "e = [[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "\n",
    "minCounts = [[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "minE = [[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "fig, axs = plt.subplots(ncols=2,sharex=True, sharey=False,\n",
    "                                        figsize=(8, 2))\n",
    "\n",
    "\n",
    "                                        \n",
    "for ki, kname in enumerate(kNames):\n",
    "    print(kname)\n",
    "    valueV = Allcounts[ki]\n",
    "    counts[ki] = np.mean(valueV)\n",
    "    e[ki] = np.std(valueV)\n",
    "    minCounts[ki] = np.mean(fullMincounts[ki])\n",
    "    minE[ki] = np.std(fullMincounts[ki])\n",
    "    print(str(np.round(np.mean(valueV, axis=0),4)) + ' \\pm ' + str(np.round(np.std(valueV, axis=0),4)))\n",
    "    print(str(np.round(np.mean(fullMincounts[ki], axis=0),4)) + ' \\pm ' + str(np.round(np.std(fullMincounts[ki], axis=0),4)))\n",
    "\n",
    "ax1 = axs[0]\n",
    "p = ax1.bar(kNames, counts, yerr=e, linestyle=\"none\", capsize=4)\n",
    "ax1.tick_params(labelrotation=90)\n",
    "ax1.set_ylim(bottom=0)\n",
    "ax1.set_ylabel('Avg. Full-DCA')\n",
    "ax1.set_title('Avg. Full-DCA')\n",
    "\n",
    "\n",
    "ax1 = axs[1]\n",
    "p = ax1.bar(kNames, minCounts, yerr=minE, linestyle=\"none\", capsize=4)\n",
    "ax1.tick_params(labelrotation=90)\n",
    "ax1.set_ylim(bottom=0)\n",
    "ax1.set_ylabel('Avg. Minimal-DCA')\n",
    "ax1.set_title('Avg. Minimal-DCA')\n",
    "\n",
    "\n",
    "\n",
    "fig.savefig(folderGeneral +'AvgDCA.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsTransformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
